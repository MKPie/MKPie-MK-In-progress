from main import SheetRow, MainWindow
from decorators import retry_on_failure
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import traceback
from webscraper_wrapper import create_webscraper_wrapper
from PyQt5.QtWidgets import QApplication, QMessageBox
from PyQt5.QtCore import QTimer
import pandas as pd
import os
import time
import re

def patched_scrape_katom(self, model_number, prefix, retries=2):
    model_number = ''.join(e for e in model_number if e.isalnum()).upper()
    if model_number.endswith("HC"):
        model_number = model_number[:-2]
    url = f"https://www.katom.com/{prefix}-{model_number}.html"
    print(f"Scraping: {url}")
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    title, description = "Title not found", "Description not found"
    specs_data = {}
    specs_html = ""
    video_links = ""
    main_image = ""
    additional_images = []
    for attempt in range(retries + 1):
        driver = None
        try:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument(f'user-agent={user_agent}')
            driver = webdriver.Chrome(options=options)
            driver.set_page_load_timeout(30)
            driver.get(url)
            if "404" in driver.title or "not found" in driver.title.lower():
                print(f"Product not found at {url}")
                return title, description, specs_data, specs_html, video_links, main_image, additional_images
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "h1.product-name.mb-0"))
            )
            title_element = driver.find_element(By.CSS_SELECTOR, "h1.product-name.mb-0")
            title = title_element.text.strip()
            if title:
                try:
                    tab_content = driver.find_element(By.CLASS_NAME, "tab-content")
                    paragraphs = tab_content.find_elements(By.TAG_NAME, "p")
                    filtered = [
                        f"<p>{p.text.strip()}</p>" for p in paragraphs
                        if p.text.strip() and not p.text.lower().startswith("*free") and "video" not in p.text.lower()
                    ]
                    description = "".join(filtered) if filtered else "Description not found"
                except NoSuchElementException:
                    print(f"Tab content not found on {url}")
                specs_data, specs_html = self.extract_table_data(driver)
                try:
                    sources = driver.find_elements(By.CSS_SELECTOR, "source[src*='.mp4'], source[type*='video']")
                    for source in sources:
                        src = source.get_attribute("src")
                        if src and src not in video_links:
                            video_links += f"{src}\n"
                    if not video_links:
                        videos = driver.find_elements(By.TAG_NAME, "video")
                        for video in videos:
                            inner_sources = video.find_elements(By.TAG_NAME, "source")
                            for source in inner_sources:
                                src = source.get_attribute("src")
                                if src and src not in video_links:
                                    video_links += f"{src}\n"
                    if not video_links:
                        page_source = driver.page_source
                        mp4_pattern = r'https?://[^"\']+\.mp4'
                        matches = re.findall(mp4_pattern, page_source)
                        for match in matches:
                            if match not in video_links:
                                video_links += f"{match}\n"
                except Exception as e:
                    print(f"Error extracting video links: {e}")
                try:
                    main_img_elem = driver.find_element(By.CSS_SELECTOR, "img.main-image, img.product-image")
                    main_image = main_img_elem.get_attribute("src")
                    img_elems = driver.find_elements(By.CSS_SELECTOR, "img.additional-image, .gallery-image")
                    additional_images = [img.get_attribute("src") for img in img_elems][:5]
                except Exception as e:
                    print(f"Image scraping error: {e}")
                break
        except Exception as e:
            print(f"Error in scrape attempt {attempt+1}: {e}")
            traceback.print_exc()
            if attempt < retries:
                retry_wait = (attempt + 1) * 2
                print(f"Retry {attempt+1}/{retries} in {retry_wait} seconds...")
                time.sleep(retry_wait)
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
    return title, description, specs_data, specs_html, video_links, main_image, additional_images

@retry_on_failure(max_attempts=4, delay=3)
def wrapped_scrape_katom(self, model_number, prefix):
    return patched_scrape_katom(self, model_number, prefix)

def patched_process_file(self):
    try:
        file_info = self.get_selected_file()
        if not file_info:
            self.signals.error.emit("No file selected")
            return
        prefix = self.prefix_input.text().strip()
        df = self.load_file_data(file_info)
        model_col = None
        for col in df.columns:
            if isinstance(col, str) and col.strip().lower() == 'mfr model':
                model_col = col
                break
        if not model_col:
            self.signals.error.emit("Missing 'Mfr Model' column in file")
            return
        config = self.parent.plugin_manager.plugins.get("Field Selector", {}).get("config", {})
        selected_fields = config.get("selected_fields", {})
        custom_fields = config.get("custom_fields", [])
        common_spec_fields = [field.replace('_', ' ').lower() for field, enabled in selected_fields.items() if enabled and field not in ["title", "description", "main_image", "additional_images"]]
        for field in custom_fields:
            if field.get("enabled", True):
                field_name = field.get("name", "").replace('_', ' ').lower()
                if field_name and field_name not in common_spec_fields:
                    common_spec_fields.append(field_name)
        columns = ["Mfr Model", "Title", "Description"]
        for field in common_spec_fields:
            columns.append(field.title())
        for i in range(1, 6):
            columns.append(f"Video Link {i}")
        if selected_fields.get("main_image", False):
            columns.append("Main Image")
        if selected_fields.get("additional_images", False):
            for i in range(1, 6):
                columns.append(f"Additional Image {i}")
        self.output_df = pd.DataFrame(columns=columns)
        output_settings = config.get("output_settings", {})
        output_dir = os.path.expanduser(output_settings.get("path", "~/GoogleDriveMount/Web/Completed/Final/"))
        output_prefix = output_settings.get("prefix", "final_")
        self.output_path = os.path.join(output_dir, f"{output_prefix}{file_info['name']}.xlsx")
        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)
        self.save_results()
        total_rows = len(df)
        if total_rows == 0:
            self.signals.error.emit("File contains no data rows")
            return
        self.signals.update_progress.emit(0, total_rows)
        for i, row_data in df.iterrows():
            if not self.running:
                break
            current_row = i + 1
            model = str(row_data[model_col])
            if not model or pd.isna(model):
                continue
            try:
                self.signals.update_status.emit(f"Processing model: {model}")
                title, desc, specs_dict, specs_html, video_links, main_image, additional_images = self.scrape_katom(model, prefix)
                if title != "Title not found" and "not found" not in title.lower():
                    combined_description = f'<div style="text-align: justify;">{desc}</div>'
                    if specs_html and len(specs_html) > 0:
                        combined_description += f'<h3 style="margin-top: 15px;">Specifications</h3>{specs_html}'
                    row_data = {
                        "Mfr Model": model,
                        "Title": title,
                        "Description": combined_description
                    }
                    for field in common_spec_fields:
                        row_data[field.title()] = ""
                    for key, value in specs_dict.items():
                        if "weight" in key.lower():
                            value = self.process_weight_value(value)
                        for field in common_spec_fields:
                            if key.lower() == field.lower() or key.lower() in field.lower():
                                row_data[field.title()] = value
                                break
                    video_list = [link.strip() for link in video_links.strip().split('\n') if link.strip()]
                    for i, link in enumerate(video_list[:5], 1):
                        row_data[f"Video Link {i}"] = link
                    for i in range(len(video_list) + 1, 6):
                        if i <= 5:
                            row_data[f"Video Link {i}"] = ""
                    if selected_fields.get("main_image", False):
                        row_data["Main Image"] = main_image
                    if selected_fields.get("additional_images", False):
                        for i, img in enumerate(additional_images, 1):
                            row_data[f"Additional Image {i}"] = img
                        for i in range(len(additional_images) + 1, 6):
                            if i <= 5:
                                row_data[f"Additional Image {i}"] = ""
                    new_row = pd.DataFrame([row_data])
                    self.output_df = pd.concat([self.output_df, new_row], ignore_index=True)
                    self.save_results()
            except Exception as e:
                print(f"Error processing row {current_row}: {e}")
                traceback.print_exc()
            self.signals.update_progress.emit(current_row, total_rows)
            time.sleep(0.5)
        if self.running:
            self.signals.finished.emit()
    except Exception as e:
        error_message = str(e)
        print(f"Error in processing: {error_message}")
        traceback.print_exc()
        self.signals.error.emit(error_message)

def patched_add_row(self):
    try:
        row = SheetRow(len(self.scroll_layout), self)
        try:
            row = create_webscraper_wrapper(row)
            print("Enhanced row with WebScraper Wrapper")
        except ImportError:
            print("WebScraper Wrapper not found, using standard row")
        except Exception as e:
            print(f"Error enhancing row: {e}")
            traceback.print_exc()
        QApplication.processEvents()
        self.scroll_layout.addWidget(row)
        QApplication.processEvents()
        QTimer.singleShot(100, lambda: self.scroll_area.verticalScrollBar().setValue(
            self.scroll_area.verticalScrollBar().maximum()))
        QTimer.singleShot(500, self.refresh_all_rows)
    except Exception as e:
        print(f"Error adding row: {e}")
        traceback.print_exc()
        QMessageBox.warning(self, "Error", f"Error adding new row: {str(e)}")

def apply_patches():
    SheetRow.scrape_katom = wrapped_scrape_katom
    SheetRow.process_file = patched_process_file
    MainWindow.add_row = patched_add_row
